---
layout: page
title: "RAG vs. Long Context: Examining Frontier Large Language Models for Environmental Review Document Comprehension"
---

Publication: arXiv preprint
Date: July 10, 2024

ðŸ“„ Paper Overview: This study examines the performance of frontier LLMs on comprehending long environmental review documents, specifically Environmental Impact Statements (EIS).

ðŸ”¬ Key Findings:
- Created NEPAQuAD1.0, the first benchmark for evaluating LLMs on EIS document question-answering
- RAG models significantly outperformed long context models in answer accuracy across LLMs
- Models performed better on closed questions compared to divergent and problem-solving questions

This work demonstrates the potential and limitations of using LLMs to assist with environmental review processes. The findings suggest RAG as a promising approach for enabling LLMs to effectively process long, domain-specific documents like EIS.

For more details, see the [full paper](https://arxiv.org/abs/2407.07321) on arXiv.